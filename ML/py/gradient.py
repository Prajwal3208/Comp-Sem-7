# -*- coding: utf-8 -*-
"""graident_decent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGfnyxIX0Wq7eSj5dYosEobizIV3uKpI
"""

import numpy as np
import matplotlib.pyplot as plt
import random

def f(x):
    return (x+3)**2

def df(x):
    return 2*(x+3)

def gradient_descent(start_x, learning_rate, num_iterations):
    x = start_x
    history_x= [x]
    for i in range(num_iterations):
        grad = df(x)
        x = x - learning_rate * grad
        history_x.append(x)
    return x,history_x

x = 2
learning_rate = 0.1
num_iterations = 50
min_x,history_x = gradient_descent(x, learning_rate, num_iterations)
print(min_x)

# plot graph of function, also of gradient descent

x = np.linspace(-10,10,100)
y = f(x)

plt.plot(x,y)

history_y = [f(i) for i in history_x]

plt.scatter(history_x,history_y)
plt.show()

# Plot the function and gradient descent steps
plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', label='f(x) = (x+3)¬≤')
plt.plot(history_x, history_y, 'ro-', label='Gradient Descent Steps')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent for f(x) = (x+3)¬≤')
plt.legend()
plt.grid(True)

plt.show()

print(f"Local minimum found at x = {min_x:.4f}")
print(f"f(x) at minimum = {f(min_x):.4f}")
# What is Gradient Descent?

# Answer: Gradient Descent is an optimization algorithm used to find the local minimum of a function by iteratively moving in the direction of the negative gradient.
# What does the learn_rate parameter do?

# Answer: It controls the step size of each iteration in gradient descent. A higher rate moves faster but may overshoot; a lower rate moves slower but is more stable.
# How does the algorithm know when to stop?

# Answer: It stops when the change in x (step_size) is less than tolerance or the maximum number of iterations (n_iter) is reached.
# What is the role of lambdify in this code?

# Answer: lambdify converts a symbolic expression (like the function or its derivative) into a callable Python function that can accept numerical input.
# Why do we use the derivative in Gradient Descent?

# Answer: The derivative gives the slope of the function, which tells us the direction to move x to decrease the function‚Äôs value.
# Explain the purpose of while step_size > tolerance in the code.

# Answer: This condition ensures the loop runs until the change in x becomes very small, indicating convergence to a local minimum.
# What happens if the learning rate is too high?

# Answer: If the learning rate is too high, the algorithm might overshoot the minimum, causing it to diverge.
# How is the gradient descent path visualized?

# Answer: matplotlib is used to plot the function, and the points generated by each iteration are plotted as markers to show the path of gradient descent.
# What is the output of the code, and what does it represent?

# Answer: The code prints the local minimum and displays a plot of the function with the gradient descent path, showing how x converges to the minimum.
# What is the role of tolerance in the code?

# Answer: tolerance is the threshold for the minimum change in x to continue iterations. It ensures that gradient descent stops when it‚Äôs close enough to the minimum.

# In the context of Gradient Descent, the algorithm iteratively adjusts 
# ùë•
# x to move towards a local minimum by following the direction of the negative gradient, which indicates where the function decreases.A local minimum is a point in a function where the function value is lower than at any nearby points. In other words, it's a "valley" in the graph of the function, although it may not be the lowest point (or global minimum) of the entire function.
