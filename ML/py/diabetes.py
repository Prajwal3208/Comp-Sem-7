
# -*- coding: utf-8 -*-
"""04_diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15XUDS1iAsCJZ3zTVk6OGxsysyEqkGi3n

# Import libraries
"""

import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from mlxtend.plotting import plot_confusion_matrix
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

"""# Data loading and preprocessing"""

df = pd.read_csv("diabetes.csv")

df

df.info()

df.describe().T

df["Outcome"].value_counts()

sns.countplot(data=df, x=df["Outcome"])
plt.show()

"""# Upsampling"""

negative_data = df[df["Outcome"] == 0]
positive_data = df[df["Outcome"] == 1]

positive_upsample = resample(positive_data,
                             replace=True,
                             n_samples=int(0.9*len(negative_data)),
                             random_state=42)

new_df = pd.concat([negative_data, positive_upsample])

# Optionally shuffle the data and reset index
new_df = new_df.sample(frac=1).reset_index(drop=True)

# Print the shape of the new dataframe
print(new_df.shape)

new_df = new_df.sample(frac=1)
sns.countplot(data=new_df, x=new_df["Outcome"])
plt.show()

x = new_df.drop("Outcome", axis=1)
y = new_df[["Outcome"]]

scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(scaled_values, y, test_size=0.2)

"""# KNN with elbow plot"""

k_values = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49]
accuracy_values = []

for i in tqdm(range(len(k_values))):
    model = KNeighborsClassifier(n_neighbors=k_values[i])
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    accuracy = metrics.accuracy_score(y_test, y_pred)
    accuracy_values.append(accuracy)

px.line(x=k_values, y=accuracy_values)

optimal_k = -1
optimal_accuracy = -1
for i in list(zip(k_values, accuracy_values)):
    if i[1] > optimal_accuracy:
        optimal_k = i[0]
        optimal_accuracy = i[1]

knn_model = KNeighborsClassifier(n_neighbors=optimal_k)

knn_model.fit(x_train, y_train)

y_pred = knn_model.predict(x_test)

print(metrics.classification_report(y_test, y_pred))

cm = metrics.confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm)
plt.show()

y_score = model.predict_proba(x_test)[:,1]

false_positive_rate, true_positive_rate, threshold = metrics.roc_curve(y_test, y_score)

print('roc_auc_score for DecisionTree: ', metrics.roc_auc_score(y_test, y_score))

plt.subplots(1, figsize=(10,7))
plt.title('Receiver Operating Characteristic - KNN')
plt.plot(false_positive_rate, true_positive_rate)
plt.plot([0, 1], ls="--")
plt.plot([0, 0], [1, 0] , c=".7"), plt.plot([1, 1] , c=".7")
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# """Here's a brief explanation of the plots and the KNN algorithm as used in your diabetes dataset implementation:

# ### K-Nearest Neighbors (KNN) Overview:
# KNN is a simple and effective supervised learning algorithm used for classification and regression tasks. It works by finding the \( k \) closest data points (neighbors) to a given test point based on a specified distance metric, usually Euclidean distance. The test point's class is then determined by a majority vote among its \( k \) nearest neighbors. KNN is particularly intuitive and works well for small datasets, but its performance can be sensitive to the choice of \( k \), distance metric, and feature scaling. In your implementation, the elbow plot is used to find the optimal value of \( k \) that balances model complexity and prediction accuracy.

# ### Plot Explanations:

# 1. **Elbow Plot for KNN**:
#    - This plot shows the relationship between different values of \( k \) (number of neighbors) and the corresponding model accuracy.
#    - As \( k \) increases, you may notice an improvement in accuracy up to a point, beyond which further increases may cause the model to become too generalized (underfitting). The point where accuracy starts to level off is often considered the "elbow" and represents the optimal value of \( k \).

# 2. **Confusion Matrix Plot**:
#    - This plot visually represents the performance of your KNN classifier by showing the counts of true positives, true negatives, false positives, and false negatives.
#    - It's a useful tool for understanding how well the model classifies each category (e.g., predicting diabetes vs. no diabetes) and helps identify where errors are occurring.

# 3. **ROC Curve**:
#    - The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classifier’s performance across different threshold values.
#    - It plots the True Positive Rate (sensitivity) against the False Positive Rate (1-specificity). A classifier that performs perfectly would have a curve that hugs the top left corner, and the area under the curve (AUC) would be 1.
#    - In your KNN implementation, the ROC curve helps evaluate how well the model distinguishes between positive and negative classes (diabetes vs. no diabetes). A higher AUC score indicates better performance.

# By using these plots, you gain insights into the performance, accuracy, and potential trade-offs of your KNN model, helping to optimize it for better predictive accuracy and interpretability.
# """
# What is the purpose of the K-Nearest Neighbors algorithm?

# Answer: K-Nearest Neighbors is a classification algorithm that assigns a data point to the most common class among its nearest neighbors.
# What does StandardScaler do?

# Answer: It scales features so that they have a mean of 0 and standard deviation of 1, which helps in ensuring that features with large values don’t dominate the KNN distance calculations.

# What does the parameter n_neighbors mean?

# Answer: n_neighbors specifies the number of nearest neighbors considered in the KNN algorithm for classifying a data point.
# What is the purpose of GridSearchCV in this code?

# Answer: GridSearchCV performs hyperparameter tuning by testing various values of n_neighbors and p to find the best parameters that maximize model accuracy.
# What is the function of the p parameter in KNeighborsClassifier?

# Answer: The p parameter determines the distance metric used. For example, p=1 uses Manhattan distance, while p=2 uses Euclidean distance.

# Explain the use of accuracy_score in the code.

# Answer: accuracy_score calculates the accuracy of predictions by comparing them to the actual labels in the test set.
# What is a confusion matrix?

# Answer: A confusion matrix is a table that describes the performance of a classification model by showing the counts of True Positives, False Positives, False Negatives, and True Negatives.
# How does KNN classify a new data point?

# Answer: KNN finds the k nearest data points (neighbors) in the training set and assigns the new point to the most common class among those neighbors.
# Why might you drop columns like BloodPressure and SkinThickness?

# Answer: Columns can be dropped to reduce noise or redundancy if they don’t contribute significantly to prediction or are correlated with other variables.
# What does classification_report provide?

# Answer: classification_report provides detailed metrics for the model, such as precision, recall, and F1-score for each class, helping to evaluate performance beyond accuracy.


