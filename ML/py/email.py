
# -*- coding: utf-8 -*-
"""email-spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14yVdDBFUsPZIHpD7ZC93afEz-fAHTeUw
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC,LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics,preprocessing

df = pd.read_csv('emails.csv')
df.head()

df.info()

df.dtypes

df.drop(columns=['Email No.'],inplace=True)

df.isna().sum()

df.isnull().sum()

df.describe()

# independent variables
x = df.iloc[:,:df.shape[1]-1]
# dependent variable
y = df.iloc[:,-1]
x.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=30)

models = {
    "K-Nearest Neighbors 2": KNeighborsClassifier(n_neighbors=2),
}

for model_name,model in models.items():
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    print(f"Accuracy of {model_name} is {metrics.accuracy_score(y_test,y_pred)}")

# """K-Nearest Neighbors (KNN) predicts the label or value of a new data point by finding the 'k' closest points in the training data. It calculates distances (e.g., Euclidean) between the new point and existing points, selects the 'k' nearest, and makes a prediction based on these neighbors. For classification, it takes a majority vote among the nearest neighbors, while for regression, it averages their values. KNN requires no training phase but can be sensitive to the choice of 'k' and data scaling, as it relies on distance-based comparisons."""
# Classification: A supervised learning approach where the goal is to assign labels to data points based on input features.
# K-Nearest Neighbors: A non-parametric method that classifies data points based on their proximity to other labeled data points.
# Support Vector Machine: A classification method that finds the hyperplane that best separates the classes in the feature space.
# Model Evaluation Metrics: Metrics like accuracy, precision, and recall help understand how well a model performs.


# How does K-Nearest Neighbors classify an email?

# KNN classifies an email based on the majority class of its nearest neighbors in the feature space.

# How does the choice of the hyperparameter 
# ùëò
# k in KNN affect the model's performance?

# A small 
# ùëò
# k can lead to a model that is sensitive to noise (overfitting), while a large 
# ùëò
# k can smooth out the predictions too much (underfitting). The optimal 
# ùëò
# k needs to be determined through cross-validation.

# What are the implications of using different kernels in SVM?

# Different kernels (linear, polynomial, RBF, sigmoid) allow SVM to capture various data distributions. The choice of kernel can significantly affect the model‚Äôs performance and its ability to generalize.

# What is cross-validation, and why is it important in model training?

# Cross-validation is a technique to evaluate the model's performance by splitting the data into multiple subsets. It helps ensure that the model generalizes well to unseen data and reduces the risk of overfitting.

# Can you explain the difference between accuracy and F1 score?

# Accuracy measures the overall correctness of the model (proportion of total correct predictions), while the F1 score is the harmonic mean of precision and recall, providing a balance between the two, especially useful in imbalanced datasets.

